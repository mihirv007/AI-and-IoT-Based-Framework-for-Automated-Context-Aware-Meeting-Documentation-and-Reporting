# -------------------------------
# Imports
# -------------------------------
import torchaudio

# ---- FIX for torchaudio >= 2.2 ----
if not hasattr(torchaudio, "set_audio_backend"):
    def _noop_backend(*args, **kwargs):
        pass
    torchaudio.set_audio_backend = _noop_backend

import whisper
from pyannote.audio import Pipeline
import spacy
import yake
import pandas as pd
import re

# -------------------------------
# Load models
# -------------------------------
print("Loading Whisper model...")
whisper_model = whisper.load_model("base", device="cpu")

print("Loading Pyannote diarization pipeline...")
pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization",
    use_auth_token=True
)

print("Loading spaCy model...")
nlp = spacy.load("en_core_web_sm")

# -------------------------------
# YAKE keyword extractor
# -------------------------------
key_word_extractor = yake.KeywordExtractor(
    lan="en",
    n=2,
    dedupLim=0.9,
    top=5
)

# -------------------------------
# Helpers
# -------------------------------
def split_into_sentences(text):
    return [sent.text.strip() for sent in nlp(text).sents]

action_verbs = [
    r'\bwill\b',
    r'\bneed\s+to\b',
    r'\bshould\b',
    r'\bmust\b',
    r'\bplan\s+to\b',
    r'\bhave\s+to\b',
    r"let['â€™]s",
    r'\blet us\b' 
]

def action_items(sentence):
    sen = sentence.lower()
    return any(re.search(p, sen) for p in action_verbs)

def extract_keywords(sentence):
    return [kw for kw, _ in key_word_extractor.extract_keywords(sentence)]

def get_speaker(diarization, start, end):
    max_overlap = 0
    speaker_label = "UNKNOWN"

    for turn, _, speaker in diarization.itertracks(yield_label=True):
        overlap = min(end, turn.end) - max(start, turn.start)
        if overlap > max_overlap:
            max_overlap = overlap
            speaker_label = speaker

    return speaker_label

# -------------------------------
# Audio path
# -------------------------------
audio_path = "amicorpus/ES2008a/audio/ES2008a.Mix-Headset.wav"

# -------------------------------
# Transcription
# -------------------------------
print("Transcribing audio...")
whisper_result = whisper_model.transcribe(audio_path)
whisper_segments = whisper_result["segments"]

# -------------------------------
# Diarization
# -------------------------------
print("Running speaker diarization...")
diarization = pipeline(audio_path, min_speakers=2, max_speakers=5)

# -------------------------------
# Final report
# -------------------------------
final_report = []

for seg in whisper_segments:
    if seg["end"] - seg["start"] > 20:
        continue

    speaker = get_speaker(diarization, seg["start"], seg["end"])
    sentences = split_into_sentences(seg["text"])

    for sentence in sentences:
        final_report.append({
            "speaker": speaker,
            "start": seg["start"],
            "end": seg["end"],
            "sentence": sentence,
            "keywords": extract_keywords(sentence),
            "action_item": action_items(sentence)
        })

# -------------------------------
# Export
# -------------------------------
df = pd.DataFrame(final_report)
df.to_csv("meeting_summary.csv", index=False)
print("Saved: meeting_summary.csv")
